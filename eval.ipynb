{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d165dc-571c-476e-97af-d0bfaed1e431",
   "metadata": {},
   "source": [
    "# Evaluation of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1419a0c-34bf-4e17-b046-3fe52a5fd12d",
   "metadata": {},
   "source": [
    "### Task breakdown:\n",
    "\n",
    "#### 1. Load Benchmark data for each query.\n",
    "\n",
    "#### 2. Load Ranked Documents for each query.\n",
    "\n",
    "#### 3. Evaluate the data\n",
    "\n",
    "#### 4. Compare the models from the results obtained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfc7c6-43a9-4b5b-9a54-f0e48cdaa75f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing code for evaluating model for 1 query with 1 model's result for that query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ca2d6-e963-4d8b-9566-722f62f594d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "For Query R101 with benchmark 'Dataset101' and result with BM25 Model for Query 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938cbf31-79d5-4cf7-ace6-7e26f33d09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load any eval data and model result to test evaluation\n",
    "query_benchmark_101_file_path = 'EvaluationBenchmark/Dataset109.txt'\n",
    "bm25model_r101_result ='RankingOutputs/BM25_R109Ranking.dat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4237c8a-fee9-40d8-861e-e4b540225546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For task 2:\n",
      "Evaluating Document ID: 16953 at Rank: 1\n",
      "At position 1, precision= 1.0, recall= 0.05\n",
      "Evaluating Document ID: 4933 at Rank: 2\n",
      "Document ID: 4933 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 64476 at Rank: 3\n",
      "At position 3, precision= 0.6666666666666666, recall= 0.1\n",
      "Evaluating Document ID: 67717 at Rank: 4\n",
      "At position 4, precision= 0.75, recall= 0.15\n",
      "Evaluating Document ID: 68812 at Rank: 5\n",
      "At position 5, precision= 0.8, recall= 0.2\n",
      "Evaluating Document ID: 23398 at Rank: 6\n",
      "Document ID: 23398 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 65289 at Rank: 7\n",
      "At position 7, precision= 0.7142857142857143, recall= 0.25\n",
      "Evaluating Document ID: 34684 at Rank: 8\n",
      "Document ID: 34684 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 26073 at Rank: 9\n",
      "At position 9, precision= 0.6666666666666666, recall= 0.3\n",
      "Evaluating Document ID: 58428 at Rank: 10\n",
      "At position 10, precision= 0.7, recall= 0.35\n",
      "Evaluating Document ID: 82229 at Rank: 11\n",
      "At position 11, precision= 0.7272727272727273, recall= 0.4\n",
      "Evaluating Document ID: 61540 at Rank: 12\n",
      "At position 12, precision= 0.75, recall= 0.45\n",
      "Evaluating Document ID: 59340 at Rank: 13\n",
      "At position 13, precision= 0.7692307692307693, recall= 0.5\n",
      "Evaluating Document ID: 58676 at Rank: 14\n",
      "At position 14, precision= 0.7857142857142857, recall= 0.55\n",
      "Evaluating Document ID: 58582 at Rank: 15\n",
      "At position 15, precision= 0.8, recall= 0.6\n",
      "Evaluating Document ID: 51139 at Rank: 16\n",
      "Document ID: 51139 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 15776 at Rank: 17\n",
      "Document ID: 15776 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 46566 at Rank: 18\n",
      "At position 18, precision= 0.7222222222222222, recall= 0.65\n",
      "Evaluating Document ID: 67144 at Rank: 19\n",
      "At position 19, precision= 0.7368421052631579, recall= 0.7\n",
      "Evaluating Document ID: 78626 at Rank: 20\n",
      "At position 20, precision= 0.75, recall= 0.75\n",
      "Evaluating Document ID: 73598 at Rank: 21\n",
      "Document ID: 73598 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 75624 at Rank: 22\n",
      "Document ID: 75624 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 56735 at Rank: 23\n",
      "At position 23, precision= 0.6956521739130435, recall= 0.8\n",
      "Evaluating Document ID: 24340 at Rank: 24\n",
      "At position 24, precision= 0.7083333333333334, recall= 0.85\n",
      "Evaluating Document ID: 16575 at Rank: 25\n",
      "Document ID: 16575 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 25832 at Rank: 26\n",
      "Document ID: 25832 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 58651 at Rank: 27\n",
      "At position 27, precision= 0.6666666666666666, recall= 0.9\n",
      "Evaluating Document ID: 61554 at Rank: 28\n",
      "Document ID: 61554 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 11402 at Rank: 29\n",
      "Document ID: 11402 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 58504 at Rank: 30\n",
      "Document ID: 58504 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 55187 at Rank: 31\n",
      "Document ID: 55187 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 56519 at Rank: 32\n",
      "Document ID: 56519 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 62293 at Rank: 33\n",
      "At position 33, precision= 0.5757575757575758, recall= 0.95\n",
      "Evaluating Document ID: 17498 at Rank: 34\n",
      "Document ID: 17498 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 49766 at Rank: 35\n",
      "Document ID: 49766 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 29314 at Rank: 36\n",
      "Document ID: 29314 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 29729 at Rank: 37\n",
      "Document ID: 29729 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 51841 at Rank: 38\n",
      "Document ID: 51841 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 51780 at Rank: 39\n",
      "Document ID: 51780 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 31530 at Rank: 40\n",
      "At position 40, precision= 0.5, recall= 1.0\n",
      "---The average precision = 0.7242655453496415\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    #import coll\n",
    "    #import df\n",
    "\n",
    "\n",
    "    # task 2 evaluation\n",
    "    # get the benchmark\n",
    "    benFile = open(query_benchmark_101_file_path)\n",
    "    #benFile = open('Training_benchmark.txt')\n",
    "    file_ = benFile.readlines()\n",
    "    ben={}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]]=float(lineList[2])\n",
    "    benFile.close()\n",
    "    \n",
    "    # Print the loaded benchmark data\n",
    "    #print(\"Loaded Benchmark Data:\")\n",
    "    #print(ben)\n",
    "    \n",
    "    # number documents \n",
    "    rank1 = {}\n",
    "    i = 1\n",
    "    for line in open(bm25model_r101_result):\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        # Strip '.xml' extension from document IDs\n",
    "        doc_id = lineList[0].replace('.xml', '')\n",
    "        rank1[str(i)] = doc_id\n",
    "        i += 1\n",
    "        \n",
    "    #Print the loaded ranked documents\n",
    "    #print(\"Loaded Ranked Documents:\")\n",
    "    #print(rank1)\n",
    "\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"For task 2:\")\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        print(\"No relevant documents in the benchmark.\")\n",
    "    else:\n",
    "        for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "            print(f\"Evaluating Document ID: {doc_id} at Rank: {n}\")\n",
    "            if doc_id in ben:\n",
    "                if ben[doc_id] > 0:\n",
    "                    ri += 1\n",
    "                    pi = float(ri) / float(int(n))\n",
    "                    recall = float(ri) / float(R)\n",
    "                    map1 += pi\n",
    "                    print(f\"At position {int(n)}, precision= {pi}, recall= {recall}\")\n",
    "                else:\n",
    "                   print(f\"Document ID: {doc_id} is found in benchmark but not relevant.\")\n",
    "            else:\n",
    "                # Instead of printing not found in benchmark for each document, count the not found documents\n",
    "                print(f\"Document ID: {doc_id} is not found in benchmark.\")\n",
    "        if ri > 0:\n",
    "            map1 = map1 / float(ri)\n",
    "        else:\n",
    "            map1 = 0\n",
    "        print(\"---The average precision = \" + str(map1))\n",
    "\n",
    "        if ri == 0:\n",
    "            print(\"No relevant documents found in the ranked results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29652e-6f06-40b4-9b56-8514a23a1270",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with BM25 model's results for those querries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3100cf0-9bc1-4013-8f60-1e174ec3a8bb",
   "metadata": {},
   "source": [
    "For queries 101 to 150 with benchmarks 101...150 for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a1a876-ecdf-431a-997a-1a1bb3a371c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Query 101: relevant documents (ri)=7, recall=1.0, MAP=0.43974180581323435\n",
      "For Query 102: relevant documents (ri)=135, recall=1.0, MAP=0.7391822372764881\n",
      "For Query 103: relevant documents (ri)=14, recall=1.0, MAP=0.3581676729274138\n",
      "For Query 104: relevant documents (ri)=120, recall=1.0, MAP=0.8882732000313384\n",
      "For Query 105: relevant documents (ri)=16, recall=1.0, MAP=0.8117570782842124\n",
      "For Query 106: relevant documents (ri)=4, recall=1.0, MAP=0.6777777777777778\n",
      "For Query 107: relevant documents (ri)=3, recall=1.0, MAP=0.4201388888888889\n",
      "For Query 108: relevant documents (ri)=3, recall=1.0, MAP=0.1799043062200957\n",
      "For Query 109: relevant documents (ri)=20, recall=1.0, MAP=0.7242655453496415\n",
      "For Query 110: relevant documents (ri)=5, recall=1.0, MAP=0.13625371360665478\n",
      "For Query 111: relevant documents (ri)=3, recall=1.0, MAP=0.15048840048840048\n",
      "For Query 112: relevant documents (ri)=6, recall=1.0, MAP=0.627020202020202\n",
      "For Query 113: relevant documents (ri)=12, recall=1.0, MAP=0.5624594326978103\n",
      "For Query 114: relevant documents (ri)=5, recall=1.0, MAP=0.805194805194805\n",
      "For Query 115: relevant documents (ri)=3, recall=1.0, MAP=0.4685714285714286\n",
      "For Query 116: relevant documents (ri)=16, recall=1.0, MAP=0.5157339762910877\n",
      "For Query 117: relevant documents (ri)=3, recall=1.0, MAP=0.7916666666666666\n",
      "For Query 118: relevant documents (ri)=3, recall=1.0, MAP=0.5277777777777778\n",
      "For Query 119: relevant documents (ri)=4, recall=1.0, MAP=0.20491452991452994\n",
      "For Query 120: relevant documents (ri)=9, recall=1.0, MAP=0.22798531271073072\n",
      "For Query 121: relevant documents (ri)=14, recall=1.0, MAP=0.5581025304796944\n",
      "For Query 122: relevant documents (ri)=15, recall=1.0, MAP=0.5230333115764084\n",
      "For Query 123: relevant documents (ri)=3, recall=1.0, MAP=0.22690217391304346\n",
      "For Query 124: relevant documents (ri)=6, recall=1.0, MAP=0.31212121212121213\n",
      "For Query 125: relevant documents (ri)=12, recall=1.0, MAP=0.6287781084656084\n",
      "For Query 126: relevant documents (ri)=19, recall=1.0, MAP=0.5546789215211195\n",
      "For Query 127: relevant documents (ri)=5, recall=1.0, MAP=0.27296703296703295\n",
      "For Query 128: relevant documents (ri)=4, recall=1.0, MAP=0.3996732026143791\n",
      "For Query 129: relevant documents (ri)=17, recall=1.0, MAP=0.5220563508000013\n",
      "For Query 130: relevant documents (ri)=3, recall=1.0, MAP=1.0\n",
      "For Query 131: relevant documents (ri)=4, recall=1.0, MAP=0.18437499999999998\n",
      "For Query 132: relevant documents (ri)=7, recall=1.0, MAP=0.4592279188098004\n",
      "For Query 133: relevant documents (ri)=5, recall=1.0, MAP=0.28296703296703296\n",
      "For Query 134: relevant documents (ri)=5, recall=1.0, MAP=0.2526613965744401\n",
      "For Query 135: relevant documents (ri)=14, recall=1.0, MAP=0.3825613838021262\n",
      "For Query 136: relevant documents (ri)=8, recall=1.0, MAP=0.3964432728219493\n",
      "For Query 137: relevant documents (ri)=3, recall=1.0, MAP=0.43333333333333335\n",
      "For Query 138: relevant documents (ri)=7, recall=1.0, MAP=0.499295839637307\n",
      "For Query 139: relevant documents (ri)=3, recall=1.0, MAP=0.5317460317460317\n",
      "For Query 140: relevant documents (ri)=11, recall=1.0, MAP=0.34160865793992734\n",
      "For Query 141: relevant documents (ri)=24, recall=1.0, MAP=0.6174236453627423\n",
      "For Query 142: relevant documents (ri)=4, recall=1.0, MAP=0.22566526610644258\n",
      "For Query 143: relevant documents (ri)=4, recall=1.0, MAP=0.11255675219089854\n",
      "For Query 144: relevant documents (ri)=6, recall=1.0, MAP=0.16628238910847606\n",
      "For Query 145: relevant documents (ri)=5, recall=1.0, MAP=0.24157125651183375\n",
      "For Query 146: relevant documents (ri)=13, recall=1.0, MAP=0.5430096854093509\n",
      "For Query 147: relevant documents (ri)=6, recall=1.0, MAP=0.38824166030048374\n",
      "For Query 148: relevant documents (ri)=12, recall=1.0, MAP=0.4671221913191727\n",
      "For Query 149: relevant documents (ri)=5, recall=1.0, MAP=0.2542857142857143\n",
      "For Query 150: relevant documents (ri)=4, recall=1.0, MAP=0.37087912087912084\n",
      "\n",
      "Overall results for 50 queries:\n",
      "Total relevant items found: 639\n",
      "Average recall: 1.0\n",
      "Average MAP: 0.44813690304147735\n",
      "\n",
      "Performance Table:\n",
      "Topic      | BM25      \n",
      "----------------------\n",
      "R101       | 0.43974180581323435\n",
      "R102       | 0.7391822372764881\n",
      "R103       | 0.3581676729274138\n",
      "R104       | 0.8882732000313384\n",
      "R105       | 0.8117570782842124\n",
      "R106       | 0.6777777777777778\n",
      "R107       | 0.4201388888888889\n",
      "R108       | 0.1799043062200957\n",
      "R109       | 0.7242655453496415\n",
      "R110       | 0.13625371360665478\n",
      "R111       | 0.15048840048840048\n",
      "R112       | 0.627020202020202\n",
      "R113       | 0.5624594326978103\n",
      "R114       | 0.805194805194805\n",
      "R115       | 0.4685714285714286\n",
      "R116       | 0.5157339762910877\n",
      "R117       | 0.7916666666666666\n",
      "R118       | 0.5277777777777778\n",
      "R119       | 0.20491452991452994\n",
      "R120       | 0.22798531271073072\n",
      "R121       | 0.5581025304796944\n",
      "R122       | 0.5230333115764084\n",
      "R123       | 0.22690217391304346\n",
      "R124       | 0.31212121212121213\n",
      "R125       | 0.6287781084656084\n",
      "R126       | 0.5546789215211195\n",
      "R127       | 0.27296703296703295\n",
      "R128       | 0.3996732026143791\n",
      "R129       | 0.5220563508000013\n",
      "R130       | 1.0       \n",
      "R131       | 0.18437499999999998\n",
      "R132       | 0.4592279188098004\n",
      "R133       | 0.28296703296703296\n",
      "R134       | 0.2526613965744401\n",
      "R135       | 0.3825613838021262\n",
      "R136       | 0.3964432728219493\n",
      "R137       | 0.43333333333333335\n",
      "R138       | 0.499295839637307\n",
      "R139       | 0.5317460317460317\n",
      "R140       | 0.34160865793992734\n",
      "R141       | 0.6174236453627423\n",
      "R142       | 0.22566526610644258\n",
      "R143       | 0.11255675219089854\n",
      "R144       | 0.16628238910847606\n",
      "R145       | 0.24157125651183375\n",
      "R146       | 0.5430096854093509\n",
      "R147       | 0.38824166030048374\n",
      "R148       | 0.4671221913191727\n",
      "R149       | 0.2542857142857143\n",
      "R150       | 0.37087912087912084\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def evaluate_query(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def print_performance_table(performance_data):\n",
    "    print(\"\\nPerformance Table:\")\n",
    "    print(f\"{'Topic':<10} | {'BM25':<10}\")\n",
    "    print(\"-\" * 22)\n",
    "    for topic, map_value in performance_data:\n",
    "        print(f\"{topic:<10} | {map_value:<10}\")\n",
    "\n",
    "def main():\n",
    "    query_range = range(101, 151)  # Adjust range as needed\n",
    "    benchmark_folder = \"EvaluationBenchmark\"\n",
    "    results_folder = \"RankingOutputs\"\n",
    "    \n",
    "    total_relevant_items = 0\n",
    "    total_recall = 0\n",
    "    total_map = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    performance_data = []\n",
    "\n",
    "    for query_id in query_range:\n",
    "        benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "        results_file = os.path.join(results_folder, f\"BM25_R{query_id}Ranking.dat\")\n",
    "        \n",
    "        if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "            ben = load_benchmark(benchmark_file)\n",
    "            rank1 = load_ranked_results(results_file)\n",
    "            ri, recall, map1 = evaluate_query(ben, rank1)\n",
    "            total_relevant_items += ri\n",
    "            total_recall += recall\n",
    "            total_map += map1\n",
    "            total_queries += 1\n",
    "            print(f\"For Query {query_id}: relevant documents (ri)={ri}, recall={recall}, MAP={map1}\")\n",
    "            performance_data.append((f\"R{query_id}\", map1))\n",
    "        else:\n",
    "            print(f\"Files for query {query_id} do not exist.\")\n",
    "    \n",
    "    if total_queries > 0:\n",
    "        avg_recall = total_recall / total_queries\n",
    "        avg_map = total_map / total_queries\n",
    "    else:\n",
    "        avg_recall = 0\n",
    "        avg_map = 0\n",
    "\n",
    "    print(f\"\\nOverall results for {total_queries} queries:\")\n",
    "    print(f\"Total relevant items found: {total_relevant_items}\")\n",
    "    print(f\"Average recall: {avg_recall}\")\n",
    "    print(f\"Average MAP: {avg_map}\")\n",
    "\n",
    "    # Call the performance table function\n",
    "    print_performance_table(performance_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc125-3bf4-4a72-8cd6-e048181c5c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with JM_LM model's results for those querries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c73a899-51fc-4e81-adea-c3af4c94c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic      BM25     JM_LM    MY_PRM\n",
      "46  R147  0.388242  0.084885  0.124556\n",
      "47  R148  0.467122  0.392474  0.522388\n",
      "48  R149  0.254286  0.251429  0.198333\n",
      "49  R150  0.370879  0.306548  0.385145\n",
      "50   MAP  0.448137  0.427711  0.311025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_26510/1501210815.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "def evaluate_map(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def create_map_performance_table(query_range, models, benchmark_folder):\n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name, results_pattern in models.items():\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            results_file = results_pattern.format(query_id=query_id)\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                rank1 = load_ranked_results(results_file)\n",
    "                ri, recall, map1 = evaluate_map(ben, rank1)\n",
    "                performance_data[f\"R{query_id}\"][model_name] = map1\n",
    "            else:\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Calculate the average MAP for each model\n",
    "    avg_map = {}\n",
    "    for model in models:\n",
    "        valid_scores = [score for score in df[model] if score != \"N/A\"]\n",
    "        if valid_scores:\n",
    "            avg_map[model] = sum(valid_scores) / len(valid_scores)\n",
    "        else:\n",
    "            avg_map[model] = \"N/A\"\n",
    "\n",
    "    # Add a row for the average MAP\n",
    "    avg_row = {'Topic': 'MAP'}\n",
    "    for model in models:\n",
    "        avg_row[model] = avg_map[model]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define the query range and models\n",
    "query_range = range(101, 151)  # Adjust range as needed\n",
    "benchmark_folder = \"EvaluationBenchmark\"\n",
    "models = {\n",
    "    \"BM25\": \"RankingOutputs/BM25_R{query_id}Ranking.dat\",\n",
    "    \"JM_LM\": \"RankingOutputs/JM_LM_R{query_id}Ranking.dat\",\n",
    "    \"MY_PRM\": \"RankingOutputs/My_PRM_R{query_id}Ranking.dat\"\n",
    "}\n",
    "\n",
    "# Create MAP Performance Table\n",
    "map_performance_df = create_map_performance_table(query_range, models, benchmark_folder)\n",
    "print(map_performance_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb222dd-0a43-4e41-8b81-8f2391ab28b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Precision@10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f85ed2-6917-4994-b8cc-f8fbc45f7292",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Precision@10 = Number of relevant documents in top 10 results by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92498d81-9ef7-4345-a563-66231a2917dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   BM25  JM_LM  MY_PRM\n",
      "0   R101  0.500  0.500   0.600\n",
      "1   R102  0.600  0.400   0.500\n",
      "2   R103  0.400  0.300   0.200\n",
      "3   R104  1.000  0.500   0.300\n",
      "4   R105  0.900  0.600   0.400\n",
      "5   R106  0.300  0.100   0.200\n",
      "6   R107  0.100  0.200   0.000\n",
      "7   R108  0.100  0.200   0.000\n",
      "8   R109  0.700  0.700   0.500\n",
      "9   R110  0.000  0.400   0.400\n",
      "10  R111  0.000  0.000   0.000\n",
      "11  R112  0.500  0.600   0.500\n",
      "12  R113  0.600  0.500   0.400\n",
      "13  R114  0.400  0.500   0.300\n",
      "14  R115  0.200  0.100   0.100\n",
      "15  R116  0.600  0.000   0.000\n",
      "16  R117  0.300  0.300   0.200\n",
      "17  R118  0.300  0.200   0.100\n",
      "18  R119  0.100  0.100   0.000\n",
      "19  R120  0.300  0.200   0.300\n",
      "20  R121  0.500  0.600   0.600\n",
      "21  R122  0.600  0.500   0.400\n",
      "22  R123  0.100  0.200   0.000\n",
      "23  R124  0.200  0.300   0.100\n",
      "24  R125  0.700  0.500   0.300\n",
      "25  R126  0.500  0.600   0.800\n",
      "26  R127  0.200  0.200   0.000\n",
      "27  R128  0.200  0.100   0.100\n",
      "28  R129  0.600  0.400   0.000\n",
      "29  R130  0.300  0.300   0.100\n",
      "30  R131  0.100  0.200   0.100\n",
      "31  R132  0.300  0.200   0.200\n",
      "32  R133  0.300  0.200   0.100\n",
      "33  R134  0.200  0.100   0.200\n",
      "34  R135  0.300  0.300   0.900\n",
      "35  R136  0.300  0.300   0.100\n",
      "36  R137  0.300  0.300   0.200\n",
      "37  R138  0.400  0.200   0.000\n",
      "38  R139  0.300  0.200   0.100\n",
      "39  R140  0.200  0.100   0.100\n",
      "40  R141  0.700  0.500   0.200\n",
      "41  R142  0.200  0.100   0.100\n",
      "42  R143  0.100  0.000   0.000\n",
      "43  R144  0.100  0.300   0.000\n",
      "44  R145  0.100  0.000   0.000\n",
      "45  R146  0.500  0.300   0.100\n",
      "46  R147  0.200  0.000   0.000\n",
      "47  R148  0.300  0.100   0.500\n",
      "48  R149  0.300  0.300   0.100\n",
      "49  R150  0.200  0.300   0.200\n",
      "50  P@10  0.344  0.282   0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_26510/2328384779.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def load_benchmark(benchmark_file):\n",
    "    \"\"\"\n",
    "    Loads benchmark data from the given file.\n",
    "\n",
    "    Args:\n",
    "        benchmark_file (str): Path to the benchmark file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with document IDs as keys and relevance scores as values.\n",
    "    \"\"\"\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_top_10_results(output_file):\n",
    "    \"\"\"\n",
    "    Loads the top 10 results from the given file.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): Path to the file containing the top 10 results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with ranks as keys and document IDs as values.\n",
    "    \"\"\"\n",
    "    top_10_rank = {}\n",
    "    with open(output_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rank, doc_id = line.split('\\t')\n",
    "            top_10_rank[rank] = doc_id\n",
    "    return top_10_rank\n",
    "\n",
    "def evaluate_precision_at_10(ben, top_10_rank):\n",
    "    \"\"\"\n",
    "    Evaluates Precision@10 for the given benchmark data and top 10 results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        top_10_rank (dict): Top 10 results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision@10 score.\n",
    "    \"\"\"\n",
    "    relevant_in_top_10 = 0\n",
    "    for rank, doc_id in top_10_rank.items():\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            relevant_in_top_10 += 1\n",
    "    precision_at_10 = relevant_in_top_10 / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def create_precision_at_10_table(query_range, models, benchmark_folder, top_10_results_folder):\n",
    "    \"\"\"\n",
    "    Creates a table with Precision@10 scores for each model and query.\n",
    "\n",
    "    Args:\n",
    "        query_range (range): Range of query IDs to evaluate.\n",
    "        models (dict): Dictionary of models with their result file patterns.\n",
    "        benchmark_folder (str): Path to the folder containing benchmark files.\n",
    "        top_10_results_folder (str): Path to the folder containing top 10 results files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing Precision@10 scores for each model and query,\n",
    "                          along with the average Precision@10 for each model.\n",
    "    \"\"\"\n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name in models:\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            top_10_file = os.path.join(top_10_results_folder, f\"{model_name}_R{query_id}_Top10.dat\")\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(top_10_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                top_10_rank = load_top_10_results(top_10_file)\n",
    "                precision_at_10 = evaluate_precision_at_10(ben, top_10_rank)\n",
    "                performance_data[f\"R{query_id}\"][model_name] = precision_at_10\n",
    "            else:\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Calculate the average Precision@10 for each model\n",
    "    avg_precision_at_10 = {}\n",
    "    for model in models:\n",
    "        valid_scores = [score for score in df[model] if score != \"N/A\"]\n",
    "        if valid_scores:\n",
    "            avg_precision_at_10[model] = sum(valid_scores) / len(valid_scores)\n",
    "        else:\n",
    "            avg_precision_at_10[model] = \"N/A\"\n",
    "\n",
    "    # Add a row for the average Precision@10\n",
    "    avg_row = {'Topic': 'P@10'}\n",
    "    for model in models:\n",
    "        avg_row[model] = avg_precision_at_10[model]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Define the query range and models\n",
    "query_range = range(101, 151)  # Adjust range as needed\n",
    "benchmark_folder = \"EvaluationBenchmark\"\n",
    "top_10_results_folder = \"Top10Results\"\n",
    "models = {\n",
    "    \"BM25\": \"My Code/RankingOutputs-New/BM25_R{query_id}Ranking.dat\",\n",
    "    \"JM_LM\": \"My Code/RankingOutputs-New/JM_LM_R{query_id}Ranking.dat\",\n",
    "    \"MY_PRM\": \"Output_Task_3_latest_30_May_arvo/My_PRM_R{query_id}Ranking.dat\"\n",
    "}\n",
    "\n",
    "# Create Precision@10 Table\n",
    "precision_at_10_df = create_precision_at_10_table(query_range, models, benchmark_folder, top_10_results_folder)\n",
    "print(precision_at_10_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a3b2d-6da3-4664-b544-483f39dba90e",
   "metadata": {},
   "source": [
    "### DCG, p=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5957e38f-978c-4d7a-8aa9-04c09ec9f28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic      BM25     JM_LM    MY_PRM\n",
      "0     R101  1.794070  2.850671  3.206878\n",
      "1     R102  2.441211  2.202318  1.891858\n",
      "2     R103  1.421247  1.087914  1.061606\n",
      "3     R104  4.543559  1.967166  0.976948\n",
      "4     R105  4.156707  2.308100  1.476948\n",
      "5     R106  2.017783  1.000000  1.289065\n",
      "6     R107  1.000000  1.017783  0.000000\n",
      "7     R108  0.386853  0.616495  0.000000\n",
      "8     R109  3.240958  3.409196  2.011435\n",
      "9     R110  0.000000  2.431960  2.253328\n",
      "10    R111  0.000000  0.000000  0.000000\n",
      "11    R112  2.237524  3.263924  2.948459\n",
      "12    R113  2.978519  2.689455  1.806847\n",
      "13    R114  2.464263  2.894940  1.279728\n",
      "14    R115  1.333333  0.500000  0.289065\n",
      "15    R116  3.070857  0.000000  0.000000\n",
      "16    R117  1.946395  1.645272  1.315465\n",
      "17    R118  1.616495  1.130930  1.000000\n",
      "18    R119  0.386853  0.315465  0.000000\n",
      "19    R120  1.090095  1.315465  1.058525\n",
      "20    R121  2.804666  3.281792  3.105696\n",
      "21    R122  2.369823  2.862636  1.422059\n",
      "22    R123  0.630930  1.315465  0.000000\n",
      "23    R124  1.017783  1.431960  0.500000\n",
      "24    R125  3.410565  1.822535  0.978605\n",
      "25    R126  1.909282  2.025777  3.841242\n",
      "26    R127  0.719741  0.671672  0.000000\n",
      "27    R128  1.289065  0.356207  0.301030\n",
      "28    R129  2.922059  1.666581  0.000000\n",
      "29    R130  2.130930  1.886853  0.430677\n",
      "30    R131  0.430677  0.817529  0.315465\n",
      "31    R132  1.919995  0.720186  1.017783\n",
      "32    R133  1.065040  1.386853  0.289065\n",
      "33    R134  0.817529  1.000000  0.930677\n",
      "34    R135  0.949828  1.789065  4.210226\n",
      "35    R136  1.394940  1.931960  0.301030\n",
      "36    R137  1.350671  2.130930  1.061606\n",
      "37    R138  2.288167  1.386853  0.000000\n",
      "38    R139  1.464263  1.430677  0.430677\n",
      "39    R140  0.817529  0.315465  0.430677\n",
      "40    R141  2.854019  2.707323  0.675918\n",
      "41    R142  0.764010  0.500000  1.000000\n",
      "42    R143  0.289065  0.000000  0.000000\n",
      "43    R144  0.289065  1.020771  0.000000\n",
      "44    R145  1.000000  0.000000  0.000000\n",
      "45    R146  2.238893  0.990571  0.333333\n",
      "46    R147  1.356207  0.000000  0.000000\n",
      "47    R148  1.464263  0.500000  2.579475\n",
      "48    R149  1.032125  1.021216  0.500000\n",
      "49    R150  1.130930  1.247425  1.130930\n",
      "50  DCG@10  1.644976  1.417307  0.993046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_26510/1253000646.py:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    \"\"\"\n",
    "    Loads benchmark data from the given file.\n",
    "\n",
    "    Args:\n",
    "        benchmark_file (str): Path to the benchmark file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with document IDs as keys and relevance scores as values.\n",
    "    \"\"\"\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_top_10_results(output_file):\n",
    "    \"\"\"\n",
    "    Loads the top 10 results from the given file.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): Path to the file containing the top 10 results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with ranks as keys and document IDs as values.\n",
    "    \"\"\"\n",
    "    top_10_rank = {}\n",
    "    with open(output_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rank, doc_id = line.split('\\t')\n",
    "            top_10_rank[rank] = doc_id\n",
    "    return top_10_rank\n",
    "\n",
    "def evaluate_dcg_at_10(ben, top_10_rank):\n",
    "    \"\"\"\n",
    "    Evaluates DCG@10 for the given benchmark data and top 10 results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        top_10_rank (dict): Top 10 results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        float: DCG@10 score.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for rank, doc_id in top_10_rank.items():\n",
    "        rank = int(rank)\n",
    "        if doc_id in ben:\n",
    "            rel = ben[doc_id]\n",
    "            if rank == 1:\n",
    "                dcg += rel\n",
    "            else:\n",
    "                dcg += rel / math.log2(rank + 1)\n",
    "    return dcg\n",
    "\n",
    "def create_dcg_at_10_table(query_range, models, benchmark_folder, top_10_results_folder):\n",
    "    \"\"\"\n",
    "    Creates a table with DCG@10 scores for each model and query.\n",
    "\n",
    "    Args:\n",
    "        query_range (range): Range of query IDs to evaluate.\n",
    "        models (dict): Dictionary of models with their result file patterns.\n",
    "        benchmark_folder (str): Path to the folder containing benchmark files.\n",
    "        top_10_results_folder (str): Path to the folder containing top 10 results files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing DCG@10 scores for each model and query,\n",
    "                          along with the average DCG@10 for each model.\n",
    "    \"\"\"\n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name in models:\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            top_10_file = os.path.join(top_10_results_folder, f\"{model_name}_R{query_id}_Top10.dat\")\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(top_10_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                top_10_rank = load_top_10_results(top_10_file)\n",
    "                dcg_at_10 = evaluate_dcg_at_10(ben, top_10_rank)\n",
    "                performance_data[f\"R{query_id}\"][model_name] = dcg_at_10\n",
    "            else:\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Calculate the average DCG@10 for each model\n",
    "    avg_dcg_at_10 = {}\n",
    "    for model in models:\n",
    "        valid_scores = [score for score in df[model] if score != \"N/A\"]\n",
    "        if valid_scores:\n",
    "            avg_dcg_at_10[model] = sum(valid_scores) / len(valid_scores)\n",
    "        else:\n",
    "            avg_dcg_at_10[model] = \"N/A\"\n",
    "\n",
    "    # Add a row for the average DCG@10\n",
    "    avg_row = {'Topic': 'DCG@10'}\n",
    "    for model in models:\n",
    "        avg_row[model] = avg_dcg_at_10[model]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Define the query range and models\n",
    "query_range = range(101, 151)  # Adjust range as needed\n",
    "benchmark_folder = \"EvaluationBenchmark\"\n",
    "top_10_results_folder = \"Top10Results\"\n",
    "models = {\n",
    "    \"BM25\": \"My Code/RankingOutputs-New/BM25_R{query_id}Ranking.dat\",\n",
    "    \"JM_LM\": \"My Code/RankingOutputs-New/JM_LM_R{query_id}Ranking.dat\",\n",
    "    \"MY_PRM\": \"Output_Task_3_latest_30_May_arvo/My_PRM_R{query_id}Ranking.dat\"\n",
    "}\n",
    "\n",
    "# Create DCG@10 Table\n",
    "dcg_at_10_df = create_dcg_at_10_table(query_range, models, benchmark_folder, top_10_results_folder)\n",
    "print(dcg_at_10_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d515023-e7e8-4320-904e-a264a580ce4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exporting results as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c4de57-7771-4fa6-af98-799d103d1f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported to PerformanceResultCSV/map_performance_df_results.csv\n",
      "DataFrame exported to PerformanceResultCSV/precision_at_10_results.csv\n",
      "DataFrame exported to PerformanceResultCSV/DCG_at_10_results.csv\n"
     ]
    }
   ],
   "source": [
    "def export_to_csv(df, filename):\n",
    "    \"\"\"\n",
    "    Exports the given DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame to export.\n",
    "        filename (str): Name of the CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"DataFrame exported to {filename}\")\n",
    "    \n",
    "    return \n",
    "\n",
    "# Export the precision  DataFrame to CSV\n",
    "export_to_csv(map_performance_df, \"PerformanceResultCSV/map_performance_df_results.csv\")\n",
    "\n",
    "# Export the Precision@10 DataFrame to CSV\n",
    "export_to_csv(precision_at_10_df, \"PerformanceResultCSV/precision_at_10_results.csv\")\n",
    "\n",
    "# Export the DCG;p=10 DataFrame to CSV\n",
    "export_to_csv(dcg_at_10_df, \"PerformanceResultCSV/DCG_at_10_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a56d64-d93b-48d7-980a-8ca2da83ec30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### evaluation system prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc5e403-32d7-4a15-8d3f-0fc00063ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Topic  BM25_MAP  BM25_P@10  BM25_DCG@10  JM_LM_MAP  JM_LM_P@10  \\\n",
      "46     R147  0.388242      0.200     1.356207   0.084885       0.000   \n",
      "47     R148  0.467122      0.300     1.464263   0.392474       0.100   \n",
      "48     R149  0.254286      0.300     1.032125   0.251429       0.300   \n",
      "49     R150  0.370879      0.200     1.130930   0.306548       0.300   \n",
      "50  Average  0.448137      0.344     1.644976   0.427711       0.282   \n",
      "\n",
      "    JM_LM_DCG@10  MY_PRM_MAP  MY_PRM_P@10  MY_PRM_DCG@10  \n",
      "46      0.000000    0.124556        0.000       0.000000  \n",
      "47      0.500000    0.522388        0.500       2.579475  \n",
      "48      1.021216    0.198333        0.100       0.500000  \n",
      "49      1.247425    0.385145        0.200       1.130930  \n",
      "50      1.417307    0.311025        0.212       0.993046  \n",
      "DataFrame exported to performance_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_26510/1089027232.py:181: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    \"\"\"\n",
    "    Loads benchmark data from the given file.\n",
    "\n",
    "    Args:\n",
    "        benchmark_file (str): Path to the benchmark file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with document IDs as keys and relevance scores as values.\n",
    "    \"\"\"\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    \"\"\"\n",
    "    Loads the ranked results from the given file.\n",
    "\n",
    "    Args:\n",
    "        results_file (str): Path to the file containing ranked results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with ranks as keys and document IDs as values.\n",
    "    \"\"\"\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "def evaluate_query_map(ben, rank1):\n",
    "    \"\"\"\n",
    "    Evaluates MAP for the given benchmark data and ranked results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        rank1 (dict): Ranked results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: ri, recall, and MAP score.\n",
    "    \"\"\"\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def evaluate_query_precision_at_10(ben, rank1):\n",
    "    \"\"\"\n",
    "    Evaluates Precision@10 for the given benchmark data and ranked results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        rank1 (dict): Ranked results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision@10 score.\n",
    "    \"\"\"\n",
    "    relevant_in_top_10 = 0\n",
    "    for n in range(1, 11):\n",
    "        if str(n) in rank1 and rank1[str(n)] in ben and ben[rank1[str(n)]] > 0:\n",
    "            relevant_in_top_10 += 1\n",
    "    return relevant_in_top_10 / 10.0\n",
    "\n",
    "def evaluate_query_dcg_at_10(ben, rank1):\n",
    "    \"\"\"\n",
    "    Evaluates DCG@10 for the given benchmark data and ranked results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        rank1 (dict): Ranked results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        float: DCG@10 score.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for rank, doc_id in rank1.items():\n",
    "        rank = int(rank)\n",
    "        if rank > 10:\n",
    "            break\n",
    "        if doc_id in ben:\n",
    "            rel = ben[doc_id]\n",
    "            if rank == 1:\n",
    "                dcg += rel\n",
    "            else:\n",
    "                dcg += rel / math.log2(rank + 1)\n",
    "    return dcg\n",
    "\n",
    "def create_performance_table(query_range, models, benchmark_folder):\n",
    "    \"\"\"\n",
    "    Creates a performance table with MAP, Precision@10, and DCG@10 scores for each model and query.\n",
    "\n",
    "    Args:\n",
    "        query_range (range): Range of query IDs to evaluate.\n",
    "        models (dict): Dictionary of models with their result file patterns.\n",
    "        benchmark_folder (str): Path to the folder containing benchmark files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing MAP, Precision@10, and DCG@10 scores for each model and query,\n",
    "                          along with the average scores for each model.\n",
    "    \"\"\"\n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name, results_pattern in models.items():\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            results_file = results_pattern.format(query_id=query_id)\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                rank1 = load_ranked_results(results_file)\n",
    "                \n",
    "                ri, recall, map1 = evaluate_query_map(ben, rank1)\n",
    "                precision_at_10 = evaluate_query_precision_at_10(ben, rank1)\n",
    "                dcg_at_10 = evaluate_query_dcg_at_10(ben, rank1)\n",
    "                \n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_MAP\"] = map1\n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_P@10\"] = precision_at_10\n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_DCG@10\"] = dcg_at_10\n",
    "            else:\n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_MAP\"] = \"N/A\"\n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_P@10\"] = \"N/A\"\n",
    "                performance_data[f\"R{query_id}\"][f\"{model_name}_DCG@10\"] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[f\"{model}_MAP\"] = []\n",
    "        df_data[f\"{model}_P@10\"] = []\n",
    "        df_data[f\"{model}_DCG@10\"] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[f\"{model}_MAP\"].append(scores.get(f\"{model}_MAP\", \"N/A\"))\n",
    "            df_data[f\"{model}_P@10\"].append(scores.get(f\"{model}_P@10\", \"N/A\"))\n",
    "            df_data[f\"{model}_DCG@10\"].append(scores.get(f\"{model}_DCG@10\", \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Calculate the average MAP, Precision@10, and DCG@10 for each model\n",
    "    avg_scores = {f\"{model}_{metric}\": \"N/A\" for model in models for metric in [\"MAP\", \"P@10\", \"DCG@10\"]}\n",
    "    for model in models:\n",
    "        for metric in [\"MAP\", \"P@10\", \"DCG@10\"]:\n",
    "            valid_scores = [score for score in df[f\"{model}_{metric}\"] if score != \"N/A\"]\n",
    "            if valid_scores:\n",
    "                avg_scores[f\"{model}_{metric}\"] = sum(valid_scores) / len(valid_scores)\n",
    "\n",
    "    # Add rows for the average MAP, Precision@10, and DCG@10\n",
    "    avg_row = {'Topic': 'Average'}\n",
    "    for model in models:\n",
    "        for metric in [\"MAP\", \"P@10\", \"DCG@10\"]:\n",
    "            avg_row[f\"{model}_{metric}\"] = avg_scores[f\"{model}_{metric}\"]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define the query range and models\n",
    "query_range = range(101, 151)  # Adjust range as needed\n",
    "benchmark_folder = \"EvaluationBenchmark\"\n",
    "models = {\n",
    "    \"BM25\": \"RankingOutputs/BM25_R{query_id}Ranking.dat\",\n",
    "    \"JM_LM\": \"RankingOutputs/JM_LM_R{query_id}Ranking.dat\",\n",
    "    \"MY_PRM\": \"RankingOutputs/My_PRM_R{query_id}Ranking.dat\"\n",
    "}\n",
    "\n",
    "# Create performance table\n",
    "performance_df = create_performance_table(query_range, models, benchmark_folder)\n",
    "print(performance_df.tail())\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "performance_df.to_csv(\"performance_results.csv\", index=False)\n",
    "print(\"DataFrame exported to performance_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b3497-f8b4-4d36-8a86-4b8568947df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
