{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d165dc-571c-476e-97af-d0bfaed1e431",
   "metadata": {},
   "source": [
    "# Evaluation of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1419a0c-34bf-4e17-b046-3fe52a5fd12d",
   "metadata": {},
   "source": [
    "### Task breakdown:\n",
    "\n",
    "#### 1. Load Benchmark data for each query.\n",
    "\n",
    "#### 2. Load Ranked Documents for each query.\n",
    "\n",
    "#### 3. Evaluate the data\n",
    "\n",
    "#### 4. Compare the models from the results obtained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfc7c6-43a9-4b5b-9a54-f0e48cdaa75f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing code for evaluating model for 1 query with 1 model's result for that query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ca2d6-e963-4d8b-9566-722f62f594d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "For Query R101 with benchmark 'Dataset101' and result with BM25 Model for Query 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "938cbf31-79d5-4cf7-ace6-7e26f33d09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_benchmark_101_file_path = 'EvaluationBenchmark/Dataset101.txt'\n",
    "bm25model_r101_result ='My Code/RankingOutputs-New/BM25_R101Ranking.dat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4237c8a-fee9-40d8-861e-e4b540225546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For task 2:\n",
      "Evaluating Document ID: 61780 at Rank: 1\n",
      "Document ID: 61780 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 63261 at Rank: 2\n",
      "At position 2, precision= 0.5, recall= 0.14285714285714285\n",
      "Evaluating Document ID: 46974 at Rank: 3\n",
      "At position 3, precision= 0.6666666666666666, recall= 0.2857142857142857\n",
      "Evaluating Document ID: 46547 at Rank: 4\n",
      "At position 4, precision= 0.75, recall= 0.42857142857142855\n",
      "Evaluating Document ID: 22170 at Rank: 5\n",
      "Document ID: 22170 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 62325 at Rank: 6\n",
      "At position 6, precision= 0.6666666666666666, recall= 0.5714285714285714\n",
      "Evaluating Document ID: 6146 at Rank: 7\n",
      "Document ID: 6146 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 22513 at Rank: 8\n",
      "Document ID: 22513 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 61329 at Rank: 9\n",
      "Document ID: 61329 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 39496 at Rank: 10\n",
      "At position 10, precision= 0.5, recall= 0.7142857142857143\n",
      "Evaluating Document ID: 77909 at Rank: 11\n",
      "Document ID: 77909 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82330 at Rank: 12\n",
      "At position 12, precision= 0.5, recall= 0.8571428571428571\n",
      "Evaluating Document ID: 18586 at Rank: 13\n",
      "Document ID: 18586 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 80950 at Rank: 14\n",
      "Document ID: 80950 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 83167 at Rank: 15\n",
      "Document ID: 83167 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82454 at Rank: 16\n",
      "At position 16, precision= 0.4375, recall= 1.0\n",
      "Evaluating Document ID: 27577 at Rank: 17\n",
      "Document ID: 27577 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 26847 at Rank: 18\n",
      "Document ID: 26847 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 30647 at Rank: 19\n",
      "Document ID: 30647 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 80425 at Rank: 20\n",
      "Document ID: 80425 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 26642 at Rank: 21\n",
      "Document ID: 26642 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 81463 at Rank: 22\n",
      "Document ID: 81463 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82912 at Rank: 23\n",
      "Document ID: 82912 is found in benchmark but not relevant.\n",
      "---The average precision = 0.5744047619047619\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    #import coll\n",
    "    #import df\n",
    "\n",
    "\n",
    "    # task 2 evaluation\n",
    "    # get the benchmark\n",
    "    benFile = open(query_benchmark_101_file_path)\n",
    "    #benFile = open('Training_benchmark.txt')\n",
    "    file_ = benFile.readlines()\n",
    "    ben={}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]]=float(lineList[2])\n",
    "    benFile.close()\n",
    "    \n",
    "    # Print the loaded benchmark data\n",
    "    #print(\"Loaded Benchmark Data:\")\n",
    "    #print(ben)\n",
    "    \n",
    "    # number documents \n",
    "    rank1 = {}\n",
    "    i = 1\n",
    "    for line in open(bm25model_r101_result):\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        # Strip '.xml' extension from document IDs\n",
    "        doc_id = lineList[0].replace('.xml', '')\n",
    "        rank1[str(i)] = doc_id\n",
    "        i += 1\n",
    "        \n",
    "    #Print the loaded ranked documents\n",
    "    #print(\"Loaded Ranked Documents:\")\n",
    "    #print(rank1)\n",
    "\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"For task 2:\")\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        print(\"No relevant documents in the benchmark.\")\n",
    "    else:\n",
    "        for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "            print(f\"Evaluating Document ID: {doc_id} at Rank: {n}\")\n",
    "            if doc_id in ben:\n",
    "                if ben[doc_id] > 0:\n",
    "                    ri += 1\n",
    "                    pi = float(ri) / float(int(n))\n",
    "                    recall = float(ri) / float(R)\n",
    "                    map1 += pi\n",
    "                    print(f\"At position {int(n)}, precision= {pi}, recall= {recall}\")\n",
    "                else:\n",
    "                   print(f\"Document ID: {doc_id} is found in benchmark but not relevant.\")\n",
    "            else:\n",
    "                # Instead of printing not found in benchmark for each document, count the not found documents\n",
    "                print(f\"Document ID: {doc_id} is not found in benchmark.\")\n",
    "        if ri > 0:\n",
    "            map1 = map1 / float(ri)\n",
    "        else:\n",
    "            map1 = 0\n",
    "        print(\"---The average precision = \" + str(map1))\n",
    "\n",
    "        if ri == 0:\n",
    "            print(\"No relevant documents found in the ranked results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29652e-6f06-40b4-9b56-8514a23a1270",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with BM25 model's results for those querries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3100cf0-9bc1-4013-8f60-1e174ec3a8bb",
   "metadata": {},
   "source": [
    "For queries 101 to 150 with benchmarks 101...150 for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1a1a876-ecdf-431a-997a-1a1bb3a371c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Query 101: relevant documents (ri)=7, recall=1.0, MAP=0.804112554112554\n",
      "For Query 102: relevant documents (ri)=135, recall=1.0, MAP=0.7014824766514469\n",
      "For Query 103: relevant documents (ri)=14, recall=1.0, MAP=0.24228592863375\n",
      "For Query 104: relevant documents (ri)=120, recall=1.0, MAP=0.6379551449146824\n",
      "For Query 105: relevant documents (ri)=16, recall=1.0, MAP=0.6098968852705267\n",
      "For Query 106: relevant documents (ri)=4, recall=1.0, MAP=0.3317550505050505\n",
      "For Query 107: relevant documents (ri)=3, recall=1.0, MAP=0.32083333333333336\n",
      "For Query 108: relevant documents (ri)=3, recall=1.0, MAP=0.19266381766381765\n",
      "For Query 109: relevant documents (ri)=20, recall=1.0, MAP=0.656323224421927\n",
      "For Query 110: relevant documents (ri)=5, recall=1.0, MAP=0.7061302681992337\n",
      "For Query 111: relevant documents (ri)=3, recall=1.0, MAP=0.10056022408963584\n",
      "For Query 112: relevant documents (ri)=6, recall=1.0, MAP=0.9583333333333334\n",
      "For Query 113: relevant documents (ri)=12, recall=1.0, MAP=0.5277799001140455\n",
      "For Query 114: relevant documents (ri)=5, recall=1.0, MAP=0.9428571428571428\n",
      "For Query 115: relevant documents (ri)=3, recall=1.0, MAP=0.1884920634920635\n",
      "For Query 116: relevant documents (ri)=16, recall=1.0, MAP=0.24272173353970794\n",
      "For Query 117: relevant documents (ri)=3, recall=1.0, MAP=0.5444444444444444\n",
      "For Query 118: relevant documents (ri)=3, recall=1.0, MAP=0.4797979797979797\n",
      "For Query 119: relevant documents (ri)=4, recall=1.0, MAP=0.1757878151260504\n",
      "For Query 120: relevant documents (ri)=9, recall=1.0, MAP=0.3190678911731543\n",
      "For Query 121: relevant documents (ri)=14, recall=1.0, MAP=0.623484171255785\n",
      "For Query 122: relevant documents (ri)=15, recall=1.0, MAP=0.6781356678324642\n",
      "For Query 123: relevant documents (ri)=3, recall=1.0, MAP=0.4551282051282051\n",
      "For Query 124: relevant documents (ri)=6, recall=1.0, MAP=0.36921296296296297\n",
      "For Query 125: relevant documents (ri)=12, recall=1.0, MAP=0.43876035306338407\n",
      "For Query 126: relevant documents (ri)=19, recall=1.0, MAP=0.5757578949888646\n",
      "For Query 127: relevant documents (ri)=5, recall=1.0, MAP=0.275974025974026\n",
      "For Query 128: relevant documents (ri)=4, recall=1.0, MAP=0.1845238095238095\n",
      "For Query 129: relevant documents (ri)=17, recall=1.0, MAP=0.4021706643102789\n",
      "For Query 130: relevant documents (ri)=3, recall=1.0, MAP=0.7555555555555555\n",
      "For Query 131: relevant documents (ri)=4, recall=1.0, MAP=0.2400691699604743\n",
      "For Query 132: relevant documents (ri)=7, recall=1.0, MAP=0.21453464671498512\n",
      "For Query 133: relevant documents (ri)=5, recall=1.0, MAP=0.41351981351981343\n",
      "For Query 134: relevant documents (ri)=5, recall=1.0, MAP=0.32660624265378085\n",
      "For Query 135: relevant documents (ri)=14, recall=1.0, MAP=0.5172015317224488\n",
      "For Query 136: relevant documents (ri)=8, recall=1.0, MAP=0.48354256854256855\n",
      "For Query 137: relevant documents (ri)=3, recall=1.0, MAP=1.0\n",
      "For Query 138: relevant documents (ri)=7, recall=1.0, MAP=0.3338957060761572\n",
      "For Query 139: relevant documents (ri)=3, recall=1.0, MAP=0.5666666666666667\n",
      "For Query 140: relevant documents (ri)=11, recall=1.0, MAP=0.2259166872102031\n",
      "For Query 141: relevant documents (ri)=24, recall=1.0, MAP=0.614460595158424\n",
      "For Query 142: relevant documents (ri)=4, recall=1.0, MAP=0.20625\n",
      "For Query 143: relevant documents (ri)=4, recall=1.0, MAP=0.12064393939393939\n",
      "For Query 144: relevant documents (ri)=6, recall=1.0, MAP=0.24613216895825593\n",
      "For Query 145: relevant documents (ri)=5, recall=1.0, MAP=0.050664093200578374\n",
      "For Query 146: relevant documents (ri)=13, recall=1.0, MAP=0.3481407880304314\n",
      "For Query 147: relevant documents (ri)=6, recall=1.0, MAP=0.0848852405456179\n",
      "For Query 148: relevant documents (ri)=12, recall=1.0, MAP=0.39247406103603105\n",
      "For Query 149: relevant documents (ri)=5, recall=1.0, MAP=0.25142857142857145\n",
      "For Query 150: relevant documents (ri)=4, recall=1.0, MAP=0.306547619047619\n",
      "\n",
      "Overall results for 50 queries:\n",
      "Total relevant items found: 639\n",
      "Average recall: 1.0\n",
      "Average MAP: 0.42771129264271546\n",
      "\n",
      "Performance Table:\n",
      "Topic      | BM25      \n",
      "----------------------\n",
      "R101       | 0.804112554112554\n",
      "R102       | 0.7014824766514469\n",
      "R103       | 0.24228592863375\n",
      "R104       | 0.6379551449146824\n",
      "R105       | 0.6098968852705267\n",
      "R106       | 0.3317550505050505\n",
      "R107       | 0.32083333333333336\n",
      "R108       | 0.19266381766381765\n",
      "R109       | 0.656323224421927\n",
      "R110       | 0.7061302681992337\n",
      "R111       | 0.10056022408963584\n",
      "R112       | 0.9583333333333334\n",
      "R113       | 0.5277799001140455\n",
      "R114       | 0.9428571428571428\n",
      "R115       | 0.1884920634920635\n",
      "R116       | 0.24272173353970794\n",
      "R117       | 0.5444444444444444\n",
      "R118       | 0.4797979797979797\n",
      "R119       | 0.1757878151260504\n",
      "R120       | 0.3190678911731543\n",
      "R121       | 0.623484171255785\n",
      "R122       | 0.6781356678324642\n",
      "R123       | 0.4551282051282051\n",
      "R124       | 0.36921296296296297\n",
      "R125       | 0.43876035306338407\n",
      "R126       | 0.5757578949888646\n",
      "R127       | 0.275974025974026\n",
      "R128       | 0.1845238095238095\n",
      "R129       | 0.4021706643102789\n",
      "R130       | 0.7555555555555555\n",
      "R131       | 0.2400691699604743\n",
      "R132       | 0.21453464671498512\n",
      "R133       | 0.41351981351981343\n",
      "R134       | 0.32660624265378085\n",
      "R135       | 0.5172015317224488\n",
      "R136       | 0.48354256854256855\n",
      "R137       | 1.0       \n",
      "R138       | 0.3338957060761572\n",
      "R139       | 0.5666666666666667\n",
      "R140       | 0.2259166872102031\n",
      "R141       | 0.614460595158424\n",
      "R142       | 0.20625   \n",
      "R143       | 0.12064393939393939\n",
      "R144       | 0.24613216895825593\n",
      "R145       | 0.050664093200578374\n",
      "R146       | 0.3481407880304314\n",
      "R147       | 0.0848852405456179\n",
      "R148       | 0.39247406103603105\n",
      "R149       | 0.25142857142857145\n",
      "R150       | 0.306547619047619\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def evaluate_query(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def print_performance_table(performance_data):\n",
    "    print(\"\\nPerformance Table:\")\n",
    "    print(f\"{'Topic':<10} | {'BM25':<10}\")\n",
    "    print(\"-\" * 22)\n",
    "    for topic, map_value in performance_data:\n",
    "        print(f\"{topic:<10} | {map_value:<10}\")\n",
    "\n",
    "def main():\n",
    "    query_range = range(101, 151)  # Adjust range as needed\n",
    "    benchmark_folder = \"EvaluationBenchmark\"\n",
    "    results_folder = \"My Code/RankingOutputs-New\"\n",
    "    \n",
    "    total_relevant_items = 0\n",
    "    total_recall = 0\n",
    "    total_map = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    performance_data = []\n",
    "\n",
    "    for query_id in query_range:\n",
    "        benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "        results_file = os.path.join(results_folder, f\"JM_LM_R{query_id}Ranking.dat\")\n",
    "        \n",
    "        if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "            ben = load_benchmark(benchmark_file)\n",
    "            rank1 = load_ranked_results(results_file)\n",
    "            ri, recall, map1 = evaluate_query(ben, rank1)\n",
    "            total_relevant_items += ri\n",
    "            total_recall += recall\n",
    "            total_map += map1\n",
    "            total_queries += 1\n",
    "            print(f\"For Query {query_id}: relevant documents (ri)={ri}, recall={recall}, MAP={map1}\")\n",
    "            performance_data.append((f\"R{query_id}\", map1))\n",
    "        else:\n",
    "            print(f\"Files for query {query_id} do not exist.\")\n",
    "    \n",
    "    if total_queries > 0:\n",
    "        avg_recall = total_recall / total_queries\n",
    "        avg_map = total_map / total_queries\n",
    "    else:\n",
    "        avg_recall = 0\n",
    "        avg_map = 0\n",
    "\n",
    "    print(f\"\\nOverall results for {total_queries} queries:\")\n",
    "    print(f\"Total relevant items found: {total_relevant_items}\")\n",
    "    print(f\"Average recall: {avg_recall}\")\n",
    "    print(f\"Average MAP: {avg_map}\")\n",
    "\n",
    "    # Call the performance table function\n",
    "    print_performance_table(performance_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc125-3bf4-4a72-8cd6-e048181c5c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with JM_LM model's results for those querries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c73a899-51fc-4e81-adea-c3af4c94c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic      BM25     JM_LM    MY_PRM\n",
      "46  R147  0.153922  0.084885  0.124556\n",
      "47  R148  0.299833  0.392474  0.522388\n",
      "48  R149  0.215439  0.251429  0.198333\n",
      "49  R150  0.259524  0.306548  0.385145\n",
      "50   MAP  0.350229  0.427711  0.315356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_15368/1058242543.py:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def evaluate_query(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def create_performance_table(performance_data, models):\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Calculate the average MAP for each model\n",
    "    avg_map = {}\n",
    "    for model in models:\n",
    "        valid_scores = [score for score in df[model] if score != \"N/A\"]\n",
    "        if valid_scores:\n",
    "            avg_map[model] = sum(valid_scores) / len(valid_scores)\n",
    "        else:\n",
    "            avg_map[model] = \"N/A\"\n",
    "\n",
    "    # Add a row for the average MAP\n",
    "    avg_row = {'Topic': 'MAP'}\n",
    "    for model in models:\n",
    "        avg_row[model] = avg_map[model]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    query_range = range(101, 151)  # Adjust range as needed\n",
    "    benchmark_folder = \"EvaluationBenchmark\"\n",
    "    models = {\n",
    "        \"BM25\": \"My Code/RankingOutputs-New/BM25_R{query_id}Ranking.dat\",\n",
    "        \"JM_LM\": \"My Code/RankingOutputs-New/JM_LM_R{query_id}Ranking.dat\",\n",
    "        \"MY_PRM\": \"Output_Task_3_latest_30_May_arvo/My_PRM_R{query_id}Ranking.dat\"\n",
    "    }\n",
    "    \n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name, results_pattern in models.items():\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            results_file = results_pattern.format(query_id=query_id)\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                rank1 = load_ranked_results(results_file)\n",
    "                ri, recall, map1 = evaluate_query(ben, rank1)\n",
    "                #print(f\"For Query {query_id} ({model_name}): relevant documents (ri)={ri}, recall={recall}, MAP={map1}\")\n",
    "                performance_data[f\"R{query_id}\"][model_name] = map1\n",
    "            else:\n",
    "                #print(f\"Files for query {query_id} ({model_name}) do not exist.\")\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    performance_df = create_performance_table(performance_data, models.keys())\n",
    "    print(performance_df.tail())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb222dd-0a43-4e41-8b81-8f2391ab28b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Precision@10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f85ed2-6917-4994-b8cc-f8fbc45f7292",
   "metadata": {},
   "source": [
    "#### Precision@10 = Number of relevant documents in top 10 results by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92498d81-9ef7-4345-a563-66231a2917dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   BM25  JM_LM  MY_PRM\n",
      "0   R101  0.500  0.500   0.600\n",
      "1   R102  0.400  0.400   0.500\n",
      "2   R103  0.200  0.300   0.200\n",
      "3   R104  0.100  0.500   0.300\n",
      "4   R105  0.500  0.600   0.400\n",
      "5   R106  0.200  0.100   0.200\n",
      "6   R107  0.200  0.200   0.000\n",
      "7   R108  0.000  0.200   0.000\n",
      "8   R109  0.200  0.700   0.500\n",
      "9   R110  0.200  0.400   0.400\n",
      "10  R111  0.000  0.000   0.000\n",
      "11  R112  0.400  0.600   0.500\n",
      "12  R113  0.500  0.500   0.400\n",
      "13  R114  0.400  0.500   0.300\n",
      "14  R115  0.200  0.100   0.100\n",
      "15  R116  0.500  0.000   0.000\n",
      "16  R117  0.300  0.300   0.200\n",
      "17  R118  0.300  0.200   0.100\n",
      "18  R119  0.100  0.100   0.000\n",
      "19  R120  0.300  0.200   0.300\n",
      "20  R121  0.300  0.600   0.600\n",
      "21  R122  0.300  0.500   0.400\n",
      "22  R123  0.200  0.200   0.000\n",
      "23  R124  0.100  0.300   0.100\n",
      "24  R125  0.700  0.500   0.300\n",
      "25  R126  0.900  0.600   0.800\n",
      "26  R127  0.400  0.200   0.000\n",
      "27  R128  0.100  0.100   0.100\n",
      "28  R129  0.000  0.400   0.100\n",
      "29  R130  0.300  0.300   0.100\n",
      "30  R131  0.300  0.200   0.100\n",
      "31  R132  0.400  0.200   0.200\n",
      "32  R133  0.200  0.200   0.100\n",
      "33  R134  0.200  0.100   0.200\n",
      "34  R135  0.500  0.300   0.900\n",
      "35  R136  0.200  0.300   0.100\n",
      "36  R137  0.300  0.300   0.200\n",
      "37  R138  0.000  0.200   0.000\n",
      "38  R139  0.200  0.200   0.100\n",
      "39  R140  0.600  0.100   0.100\n",
      "40  R141  0.300  0.500   0.200\n",
      "41  R142  0.100  0.100   0.100\n",
      "42  R143  0.100  0.000   0.000\n",
      "43  R144  0.100  0.300   0.000\n",
      "44  R145  0.200  0.000   0.000\n",
      "45  R146  0.100  0.300   0.100\n",
      "46  R147  0.000  0.000   0.000\n",
      "47  R148  0.200  0.100   0.500\n",
      "48  R149  0.200  0.300   0.100\n",
      "49  R150  0.200  0.300   0.200\n",
      "50  P@10  0.264  0.282   0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w120cf1j2bb4s34_0vnqtdp80000gn/T/ipykernel_15368/269471394.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(avg_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def load_benchmark(benchmark_file):\n",
    "    \"\"\"\n",
    "    Loads benchmark data from the given file.\n",
    "\n",
    "    Args:\n",
    "        benchmark_file (str): Path to the benchmark file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with document IDs as keys and relevance scores as values.\n",
    "    \"\"\"\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_top_10_results(output_file):\n",
    "    \"\"\"\n",
    "    Loads the top 10 results from the given file.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): Path to the file containing the top 10 results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with ranks as keys and document IDs as values.\n",
    "    \"\"\"\n",
    "    top_10_rank = {}\n",
    "    with open(output_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rank, doc_id = line.split('\\t')\n",
    "            top_10_rank[rank] = doc_id\n",
    "    return top_10_rank\n",
    "\n",
    "def evaluate_precision_at_10(ben, top_10_rank):\n",
    "    \"\"\"\n",
    "    Evaluates Precision@10 for the given benchmark data and top 10 results.\n",
    "\n",
    "    Args:\n",
    "        ben (dict): Benchmark data with document IDs as keys and relevance scores as values.\n",
    "        top_10_rank (dict): Top 10 results with ranks as keys and document IDs as values.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision@10 score.\n",
    "    \"\"\"\n",
    "    relevant_in_top_10 = 0\n",
    "    for rank, doc_id in top_10_rank.items():\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            relevant_in_top_10 += 1\n",
    "    precision_at_10 = relevant_in_top_10 / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def create_precision_at_10_table(query_range, models, benchmark_folder, top_10_results_folder):\n",
    "    \"\"\"\n",
    "    Creates a table with Precision@10 scores for each model and query.\n",
    "\n",
    "    Args:\n",
    "        query_range (range): Range of query IDs to evaluate.\n",
    "        models (dict): Dictionary of models with their result file patterns.\n",
    "        benchmark_folder (str): Path to the folder containing benchmark files.\n",
    "        top_10_results_folder (str): Path to the folder containing top 10 results files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing Precision@10 scores for each model and query,\n",
    "                          along with the average Precision@10 for each model.\n",
    "    \"\"\"\n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name in models:\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            top_10_file = os.path.join(top_10_results_folder, f\"{model_name}_R{query_id}_Top10.dat\")\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(top_10_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                top_10_rank = load_top_10_results(top_10_file)\n",
    "                precision_at_10 = evaluate_precision_at_10(ben, top_10_rank)\n",
    "                performance_data[f\"R{query_id}\"][model_name] = precision_at_10\n",
    "            else:\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Calculate the average Precision@10 for each model\n",
    "    avg_precision_at_10 = {}\n",
    "    for model in models:\n",
    "        valid_scores = [score for score in df[model] if score != \"N/A\"]\n",
    "        if valid_scores:\n",
    "            avg_precision_at_10[model] = sum(valid_scores) / len(valid_scores)\n",
    "        else:\n",
    "            avg_precision_at_10[model] = \"N/A\"\n",
    "\n",
    "    # Add a row for the average Precision@10\n",
    "    avg_row = {'Topic': 'P@10'}\n",
    "    for model in models:\n",
    "        avg_row[model] = avg_precision_at_10[model]\n",
    "\n",
    "    df = df.append(avg_row, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Define the query range and models\n",
    "query_range = range(101, 151)  # Adjust range as needed\n",
    "benchmark_folder = \"EvaluationBenchmark\"\n",
    "top_10_results_folder = \"Top10Results\"\n",
    "models = {\n",
    "    \"BM25\": \"My Code/RankingOutputs-New/BM25_R{query_id}Ranking.dat\",\n",
    "    \"JM_LM\": \"My Code/RankingOutputs-New/JM_LM_R{query_id}Ranking.dat\",\n",
    "    \"MY_PRM\": \"Output_Task_3_latest_30_May_arvo/My_PRM_R{query_id}Ranking.dat\"\n",
    "}\n",
    "\n",
    "# Create Precision@10 Table\n",
    "precision_at_10_df = create_precision_at_10_table(query_range, models, benchmark_folder, top_10_results_folder)\n",
    "print(precision_at_10_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957e38f-978c-4d7a-8aa9-04c09ec9f28b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
