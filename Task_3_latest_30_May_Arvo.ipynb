{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d494a7a-17e5-4edb-8560-44a80ce3a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jcrispii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jcrispii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98441cf7-c98a-404c-a24d-6cdba9d57d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setup\n",
    "data_directory = 'Data_Collection-1/Data_Collection'\n",
    "query_file_path = 'the50Queries.txt'\n",
    "output_directory = 'Output_Task_3_latest_30_May_arvo'\n",
    "\n",
    "# Parameters\n",
    "N_top_docs = 5  # Number of top documents to use for query expansion\n",
    "\n",
    "# Stopwords and stemmer setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [stemmer.stem(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "\n",
    "def load_queries(query_file_path):\n",
    "    queries = {}\n",
    "    with open(query_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        pattern = re.compile(r'<num> Number: (R\\d+).*?<title>(.*?)\\n', re.DOTALL)\n",
    "        for match in pattern.finditer(content):\n",
    "            query_id, title = match.groups()\n",
    "            queries[query_id] = process_text(title)\n",
    "    return queries\n",
    "\n",
    "def load_documents(data_directory, query_id):\n",
    "    document_path = os.path.join(data_directory, f\"Data_C{query_id[1:]}\")  # Adjust query_id if needed\n",
    "    if not os.path.exists(document_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {document_path}\")\n",
    "    documents = {}\n",
    "    for filename in os.listdir(document_path):\n",
    "        with open(os.path.join(document_path, filename), 'r', encoding='utf-8') as file:\n",
    "            documents[filename] = process_text(file.read())\n",
    "    return documents\n",
    "\n",
    "def calculate_tfidf(documents, query):\n",
    "    tfidf_scores = defaultdict(float)\n",
    "    doc_count = len(documents)\n",
    "    doc_freq = Counter(word for doc in documents.values() for word in set(doc))\n",
    "\n",
    "    for doc_id, content in documents.items():\n",
    "        term_freq = Counter(content)\n",
    "        doc_length = len(content)\n",
    "        for term in query:\n",
    "            tf = term_freq[term] / doc_length\n",
    "            idf = math.log(doc_count / (1 + doc_freq[term]) + 1)\n",
    "            tfidf_scores[doc_id] += tf * idf\n",
    "\n",
    "    return tfidf_scores\n",
    "\n",
    "def expand_query(top_documents, documents):\n",
    "    term_frequency = Counter(word for doc_id in top_documents for word in documents[doc_id])\n",
    "    most_common_terms = [word for word, freq in term_frequency.most_common(10)]\n",
    "    return most_common_terms\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    queries = load_queries(query_file_path)\n",
    "\n",
    "    for query_id, query_tokens in queries.items():\n",
    "        documents = load_documents(data_directory, query_id)\n",
    "        initial_scores = calculate_tfidf(documents, query_tokens)\n",
    "        top_documents = sorted(initial_scores, key=initial_scores.get, reverse=True)[:N_top_docs]\n",
    "        expanded_query = expand_query(top_documents, documents)\n",
    "        final_scores = calculate_tfidf(documents, query_tokens + expanded_query)\n",
    "\n",
    "        output_path = os.path.join(output_directory, f\"My_PRM_{query_id}Ranking.dat\")\n",
    "        with open(output_path, 'w') as file:\n",
    "            for doc_id in sorted(final_scores, key=final_scores.get, reverse=True):\n",
    "                file.write(f\"{doc_id}\\t{final_scores[doc_id]}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900669e-3e97-4595-b081-1a05e41dd0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
